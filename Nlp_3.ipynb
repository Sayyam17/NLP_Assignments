{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7-yXIURyieV",
        "outputId": "3a396043-d296-4ad0-f7bf-e706e7199416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL DATASET\n",
            "                                  text     label\n",
            "0             I love Machine Learning!  positive\n",
            "1   Machine learning is very powerful.  positive\n",
            "2     Deep learning is a branch of ML.   neutral\n",
            "3  I enjoy learning new AI techniques.  positive\n",
            "\n",
            "CLEANED TEXT\n",
            "                                  text                     cleaned_text\n",
            "0             I love Machine Learning!            love machine learning\n",
            "1   Machine learning is very powerful.        machine learning powerful\n",
            "2     Deep learning is a branch of ML.          deep learning branch ml\n",
            "3  I enjoy learning new AI techniques.  enjoy learning new ai technique\n",
            "\n",
            "LABEL ENCODING\n",
            "      label  label_encoded\n",
            "0  positive              1\n",
            "1  positive              1\n",
            "2   neutral              0\n",
            "3  positive              1\n",
            "\n",
            "TF-IDF MATRIX\n",
            "         ai    branch      deep     enjoy  learning      love   machine  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.379192  0.726641  0.572892   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.379192  0.000000  0.572892   \n",
            "2  0.000000  0.552805  0.552805  0.000000  0.288477  0.000000  0.000000   \n",
            "3  0.483803  0.000000  0.000000  0.483803  0.252468  0.000000  0.000000   \n",
            "\n",
            "         ml       new  powerful  technique  \n",
            "0  0.000000  0.000000  0.000000   0.000000  \n",
            "1  0.000000  0.000000  0.726641   0.000000  \n",
            "2  0.552805  0.000000  0.000000   0.000000  \n",
            "3  0.000000  0.483803  0.000000   0.483803  \n",
            "\n",
            "FILES SAVED SUCCESSFULLY:\n",
            "1. cleaned_and_encoded_data.csv\n",
            "2. tfidf_features.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love Machine Learning!\",\n",
        "        \"Machine learning is very powerful.\",\n",
        "        \"Deep learning is a branch of ML.\",\n",
        "        \"I enjoy learning new AI techniques.\"\n",
        "    ],\n",
        "    \"label\": [\"positive\", \"positive\", \"neutral\", \"positive\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"ORIGINAL DATASET\")\n",
        "print(df)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word)\n",
        "             for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "print(\"\\nCLEANED TEXT\")\n",
        "print(df[['text', 'cleaned_text']])\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "print(\"\\nLABEL ENCODING\")\n",
        "print(df[['label', 'label_encoded']])\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\nTF-IDF MATRIX\")\n",
        "print(tfidf_df)\n",
        "\n",
        "\n",
        "df.to_csv(\"cleaned_and_encoded_data.csv\", index=False)\n",
        "tfidf_df.to_csv(\"tfidf_features.csv\", index=False)\n",
        "\n",
        "print(\"\\nFILES SAVED SUCCESSFULLY:\")\n",
        "print(\"1. cleaned_and_encoded_data.csv\")\n",
        "print(\"2. tfidf_features.csv\")\n"
      ]
    }
  ]
}