{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbftFNE2tbo2",
        "outputId": "6d59d724-b36b-43a5-ffd1-7bd0c79731e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ORIGINAL DOCUMENTS =====\n",
            "1. I love machine learning\n",
            "2. Machine learning is fun\n",
            "3. I love deep learning\n",
            "4. Deep learning is powerful\n",
            "\n",
            "===== BAG OF WORDS (COUNT) =====\n",
            "   deep  fun  is  learning  love  machine  powerful\n",
            "0     0    0   0         1     1        1         0\n",
            "1     0    1   1         1     0        1         0\n",
            "2     1    0   0         1     1        0         0\n",
            "3     1    0   1         1     0        0         1\n",
            "\n",
            "===== NORMALIZED BAG OF WORDS (L1) =====\n",
            "       deep   fun    is  learning      love   machine  powerful\n",
            "0  0.000000  0.00  0.00  0.333333  0.333333  0.333333      0.00\n",
            "1  0.000000  0.25  0.25  0.250000  0.000000  0.250000      0.00\n",
            "2  0.333333  0.00  0.00  0.333333  0.333333  0.000000      0.00\n",
            "3  0.250000  0.00  0.25  0.250000  0.000000  0.000000      0.25\n",
            "\n",
            "===== TF-IDF =====\n",
            "       deep       fun        is  learning      love   machine  powerful\n",
            "0  0.000000  0.000000  0.000000  0.423897  0.640434  0.640434  0.000000\n",
            "1  0.000000  0.630504  0.497096  0.329023  0.000000  0.497096  0.000000\n",
            "2  0.640434  0.000000  0.000000  0.423897  0.640434  0.000000  0.000000\n",
            "3  0.497096  0.000000  0.497096  0.329023  0.000000  0.000000  0.630504\n",
            "\n",
            "===== WORD2VEC EMBEDDING (sample word: 'learning') ====\n",
            "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n",
            "\n",
            "===== DOCUMENT EMBEDDING SHAPE =====\n",
            "(4, 50)\n",
            "\n",
            "===== DOCUMENT EMBEDDING (Doc 1) ====\n",
            "[ 0.00440647 -0.00442122 -0.0044943   0.00164885 -0.00584756  0.0013381\n",
            "  0.00497447  0.00691416 -0.01025242  0.0032176  -0.000751   -0.00261476\n",
            "  0.00425746  0.00193624 -0.00051108  0.00345703  0.01171891  0.00532074\n",
            " -0.00807905 -0.00106817  0.00027311 -0.00167633  0.00633625  0.00033809\n",
            "  0.00517439 -0.00330266  0.00500173  0.00616334 -0.0058827  -0.00181817\n",
            " -0.00509634 -0.00649408 -0.00060353 -0.00822699 -0.00425682 -0.00566879\n",
            "  0.00148657 -0.0099304   0.00625383 -0.00513198 -0.00930968  0.00543157\n",
            "  0.00085987 -0.00478979  0.00459299  0.00613596  0.00249944  0.00251798\n",
            " -0.00034088  0.00284395]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize # Import normalize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is fun\",\n",
        "    \"I love deep learning\",\n",
        "    \"Deep learning is powerful\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== ORIGINAL DOCUMENTS =====\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"{i+1}. {doc}\")\n",
        "\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow = bow_vectorizer.fit_transform(documents)\n",
        "\n",
        "bow_df = pd.DataFrame(\n",
        "    bow.toarray(),\n",
        "    columns=bow_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\n===== BAG OF WORDS (COUNT) =====\")\n",
        "print(bow_df)\n",
        "\n",
        "\n",
        "bow_norm_vectorizer = CountVectorizer()\n",
        "bow_raw_counts = bow_norm_vectorizer.fit_transform(documents)\n",
        "bow_norm = normalize(bow_raw_counts, norm='l1', axis=1)\n",
        "\n",
        "bow_norm_df = pd.DataFrame(\n",
        "    bow_norm.toarray(),\n",
        "    columns=bow_norm_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\n===== NORMALIZED BAG OF WORDS (L1) =====\")\n",
        "print(bow_norm_df)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\n===== TF-IDF =====\")\n",
        "print(tfidf_df)\n",
        "\n",
        "\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=50,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "print(\"\\n===== WORD2VEC EMBEDDING (sample word: 'learning') ====\")\n",
        "print(w2v_model.wv['learning'])\n",
        "\n",
        "\n",
        "def document_embedding(doc):\n",
        "    words = doc.lower().split()\n",
        "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "doc_embeddings = np.array([document_embedding(doc) for doc in documents])\n",
        "\n",
        "print(\"\\n===== DOCUMENT EMBEDDING SHAPE =====\")\n",
        "print(doc_embeddings.shape)\n",
        "\n",
        "print(\"\\n===== DOCUMENT EMBEDDING (Doc 1) ====\")\n",
        "print(doc_embeddings[0])"
      ]
    }
  ]
}